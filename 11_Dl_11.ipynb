{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4a248256",
      "metadata": {
        "id": "4a248256"
      },
      "source": [
        "#### 1.\tWrite the Python code to implement a single neuron."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e73cae6",
      "metadata": {
        "id": "1e73cae6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Neuron:\n",
        "    def __init__(self, num_inputs):\n",
        "        self.weights = np.random.randn(num_inputs)\n",
        "        self.bias = np.random.randn()\n",
        "\n",
        "    def activate(self, inputs):\n",
        "        # Weighted sum of inputs\n",
        "        weighted_sum = np.dot(inputs, self.weights) + self.bias\n",
        "\n",
        "        # Activation function (e.g., sigmoid)\n",
        "        activation = 1 / (1 + np.exp(-weighted_sum))\n",
        "\n",
        "        return activation\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a52d2e0d",
      "metadata": {
        "id": "a52d2e0d"
      },
      "outputs": [],
      "source": [
        "num_inputs = 3\n",
        "inputs = np.array([0.5, 0.3, 0.8])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25f4bbc0",
      "metadata": {
        "id": "25f4bbc0"
      },
      "outputs": [],
      "source": [
        "neuron = Neuron(num_inputs)\n",
        "output = neuron.activate(inputs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12d27235",
      "metadata": {
        "id": "12d27235",
        "outputId": "1fafbe4c-a92b-4828-d03a-b51da33e8244"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7570785847735598"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5c5f184",
      "metadata": {
        "id": "f5c5f184"
      },
      "source": [
        "#### 2.\tWrite the Python code to implement ReLU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4315a679",
      "metadata": {
        "id": "4315a679",
        "outputId": "0570b1fa-ba16-4f70-f04a-f43a650b97b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "print(relu(20))\n",
        "print(relu(-20))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edf7aaa4",
      "metadata": {
        "id": "edf7aaa4"
      },
      "source": [
        "#### 3.\tWrite the Python code for a dense layer in terms of matrix multiplication."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a487ef2",
      "metadata": {
        "id": "2a487ef2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class DenseLayer:\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.weights = np.random.randn(output_size, input_size)\n",
        "        self.bias = np.random.randn(output_size, 1)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Perform matrix multiplication between inputs and weights\n",
        "        weighted_sum = np.dot(self.weights, inputs)\n",
        "\n",
        "        # Add bias to the weighted sum\n",
        "        weighted_sum_with_bias = weighted_sum + self.bias\n",
        "\n",
        "        # Apply activation function\n",
        "        activation = self._activation_function(weighted_sum_with_bias)\n",
        "\n",
        "        return activation\n",
        "\n",
        "    def _activation_function(self, x):\n",
        "        #activation function (sigmoid)\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25e4f1f3",
      "metadata": {
        "id": "25e4f1f3"
      },
      "outputs": [],
      "source": [
        "input_size = 3\n",
        "output_size = 2\n",
        "inputs = np.array([0.5, 0.3, 0.8])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c66842eb",
      "metadata": {
        "id": "c66842eb",
        "outputId": "6fa5c4e6-03d5-457b-e9fe-5cd117bf91c7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.84712347, 0.51178247],\n",
              "       [0.5324487 , 0.17724876]])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dense_layer = DenseLayer(input_size, output_size)\n",
        "dense_layer.forward(inputs) # will print output of dense layer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "746cb7cc",
      "metadata": {
        "id": "746cb7cc"
      },
      "source": [
        "#### 4.\tWrite the Python code for a dense layer in plain Python (that is, with list comprehensions and functionality built into Python)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "802c6adb",
      "metadata": {
        "id": "802c6adb"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import math\n",
        "\n",
        "class DenseLayer:\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.weights = [[random.random() for _ in range(input_size)] for _ in range(output_size)]\n",
        "        self.bias = [random.random() for _ in range(output_size)]\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Perform matrix multiplication between inputs and weights\n",
        "        weighted_sum = [sum(x * w for x, w in zip(inputs, weights)) for weights in self.weights]\n",
        "\n",
        "        # Add bias to the weighted sum\n",
        "        weighted_sum_with_bias = [x + b for x, b in zip(weighted_sum, self.bias)]\n",
        "\n",
        "        # Apply activation function\n",
        "        activation = [self._activation_function(x) for x in weighted_sum_with_bias]\n",
        "\n",
        "        return activation\n",
        "\n",
        "    def _activation_function(self, x):\n",
        "        # Example activation function (e.g., sigmoid)\n",
        "        return 1 / (1 + math.exp(-x))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5004a8c",
      "metadata": {
        "id": "e5004a8c"
      },
      "outputs": [],
      "source": [
        "\n",
        "input_size = 3\n",
        "output_size = 2\n",
        "inputs = [0.5, 0.3, 0.8]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4171cbea",
      "metadata": {
        "id": "4171cbea",
        "outputId": "57b2bef1-e64d-46e7-d6f6-95f495df8f93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dense layer output: [[0.59759666 0.88404977]\n",
            " [0.11074537 0.39001271]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "dense_layer = DenseLayer(input_size, output_size)\n",
        "output = dense_layer.forward(inputs)\n",
        "\n",
        "print(\"Dense layer output:\", output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "332cbd41",
      "metadata": {
        "id": "332cbd41"
      },
      "source": [
        "#### 5.\tWhat is the “hidden size” of a layer?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbf0f5db",
      "metadata": {
        "id": "bbf0f5db"
      },
      "source": [
        "The \"hidden size\" of a layer refers to the number of neurons or units present in that layer. In a neural network, a layer consists of a collection of neurons that perform computations on the input data. Each neuron in a layer takes in input values, applies weights to those inputs, computes a weighted sum, and then applies an activation function to produce an output.\n",
        "\n",
        "The hidden size determines the capacity or complexity of the layer. It represents the number of independent parameters or degrees of freedom that the layer has to learn from the data. A higher hidden size allows the layer to learn more complex patterns and representations in the data, but it also increases the computational and memory requirements of the layer."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e479268",
      "metadata": {
        "id": "4e479268"
      },
      "source": [
        "#### 6.\tWhat does the t method do in PyTorch?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "351a278d",
      "metadata": {
        "id": "351a278d"
      },
      "source": [
        "In PyTorch, the t() method is used to transpose a tensor."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f71bd58a",
      "metadata": {
        "id": "f71bd58a"
      },
      "source": [
        "#### 7.\tWhy is matrix multiplication written in plain Python very slow?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98e1b33c",
      "metadata": {
        "id": "98e1b33c"
      },
      "source": [
        "Matrix multiplication involves a large number of arithmetic operations, and performing those operations using the basic Python operations can be slow because Python is an interpreted language, which means that it has to interpret and execute each line of code at runtime. This can lead to a significant overhead when working with large matrices."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d16db75",
      "metadata": {
        "id": "0d16db75"
      },
      "source": [
        "#### 8.\tIn matmul, why is ac==br?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1ff3032",
      "metadata": {
        "id": "a1ff3032"
      },
      "source": [
        "In matrix multiplication (matmul), the condition for valid multiplication is that the number of columns in the left matrix must be equal to the number of rows in the right matrix. This condition is expressed as `ac == br`, where `ac` represents the number of columns in the left matrix and `br` represents the number of rows in the right matrix.\n",
        "\n",
        "If the condition `ac == br` is not satisfied, the matmul operation will result in an error. However, when the condition is met, the resulting matrix will have dimensions `(ar, bc)`, where `ar` is the number of rows in the left matrix and `bc` is the number of columns in the right matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "979e3af2",
      "metadata": {
        "id": "979e3af2"
      },
      "source": [
        "#### 9.\tIn Jupyter Notebook, how do you measure the time taken for a single cell to execute?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a8061e0",
      "metadata": {
        "id": "4a8061e0"
      },
      "source": [
        "To measure the time taken for a single cell to execute in a Jupyter Notebook, you can use the %%timeit magic command. This command runs the cell multiple times and provides the average time taken for the cell to execute."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd77bd31",
      "metadata": {
        "id": "bd77bd31"
      },
      "source": [
        "#### 10.\tWhat is elementwise arithmetic?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44baa46e",
      "metadata": {
        "id": "44baa46e"
      },
      "source": [
        " Elementwise arithmetic refers to performing arithmetic operations on the corresponding elements of two or more arrays or matrices. In elementwise arithmetic, each element of one array is paired with the corresponding element of another array, and the arithmetic operation is performed on those pairs of elements."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9b2fdc0",
      "metadata": {
        "id": "c9b2fdc0"
      },
      "source": [
        "#### 11.\tWrite the PyTorch code to test whether every element of a is greater than the corresponding element of b."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "52c535d9",
      "metadata": {
        "id": "52c535d9"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "57d8ceb1",
      "metadata": {
        "id": "57d8ceb1"
      },
      "outputs": [],
      "source": [
        "a = torch.tensor([1, 2, 3])\n",
        "b = torch.tensor([0, 2, 2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "04f77793",
      "metadata": {
        "id": "04f77793"
      },
      "outputs": [],
      "source": [
        "result = torch.gt(a, b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6f58126f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6f58126f",
        "outputId": "686b09a2-6867-4e1b-f760-d803cbc4e27c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not all elements of a are greater than b\n",
            "Indices where the condition fails:  (tensor([1]),)\n"
          ]
        }
      ],
      "source": [
        "if torch.all(result):\n",
        "    print(\"All elements of a are greater than b\")\n",
        "else:\n",
        "    print(\"Not all elements of a are greater than b\")\n",
        "    print(\"Indices where the condition fails: \", torch.where(result == False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d03ba3dd",
      "metadata": {
        "id": "d03ba3dd"
      },
      "source": [
        "#### 12.\tWhat is a rank-0 tensor? How do you convert it to a plain Python data type?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "caf7caf1",
      "metadata": {
        "id": "caf7caf1"
      },
      "source": [
        "A rank-0 tensor is a tensor with zero dimensions, also known as a scalar or a 0-d tensor. It represents a single numerical value, such as an integer or a floating-point number."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "c0accda4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0accda4",
        "outputId": "d93c6161-9427-4a69-ca58-0ebad8a72ab0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(42)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "x = torch.tensor(42)\n",
        "print(x) #This creates a rank-0 tensor x with the value of 42."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ce29b56a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce29b56a",
        "outputId": "68c22a44-02f4-4b61-a05a-1a72d203d508"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'int'>\n",
            "42\n"
          ]
        }
      ],
      "source": [
        "y = x.item()\n",
        "print(type(y))\n",
        "print(y) # y is assigned the value of 42 as a plain Python integer. The type(y) call confirms that y is a Python integer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69e79117",
      "metadata": {
        "id": "69e79117"
      },
      "source": [
        "#### 13.\tHow does elementwise arithmetic help us speed up matmul?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81f62446",
      "metadata": {
        "id": "81f62446"
      },
      "source": [
        "Element-wise arithmetic can enhance the efficiency of matrix multiplication by minimizing the number of operations needed to compute the output. This is particularly useful in the context of \"broadcasting matrix multiplication,\" which is a faster variant of matrix multiplication.\n",
        "\n",
        "In broadcasting matrix multiplication, the two input matrices are expanded or \"broadcasted\" to have the same shape. By doing so, we can perform element-wise multiplication between corresponding elements of the matrices. Afterward, a summation is performed along a specific axis to obtain the final result. This approach reduces the computational complexity and improves the speed of the matrix multiplication operation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe43354d",
      "metadata": {
        "id": "fe43354d"
      },
      "source": [
        "#### 14.\tWhat are the broadcasting rules?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "936671ab",
      "metadata": {
        "id": "936671ab"
      },
      "source": [
        "Broadcasting rules in mathematics and array operations define how arrays with different shapes can be combined or operated upon. These rules allow for element-wise operations between arrays of different shapes without explicitly duplicating or reshaping the arrays.\n",
        "\n",
        "The broadcasting rules are as follows:\n",
        "\n",
        "1. If the arrays have the same number of dimensions, but their sizes differ in at least one dimension, the array with size 1 in that dimension is stretched or repeated to match the size of the other array along that dimension.\n",
        "\n",
        "2. If one of the arrays has fewer dimensions than the other, the array with fewer dimensions is implicitly padded with size 1 dimensions on its left until both arrays have the same number of dimensions.\n",
        "\n",
        "3. If the size of any dimension in the two arrays does not match and is not 1, an error is raised. The sizes are incompatible and cannot be broadcasted.\n",
        "\n",
        "4. After applying the broadcasting rules, if the shapes of the arrays are still not compatible, an error is raised. The shapes must be compatible for element-wise operations to be valid.\n",
        "\n",
        "These broadcasting rules enable efficient and concise operations on arrays with different shapes, reducing the need for explicit reshaping or duplication of data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b5204b7",
      "metadata": {
        "id": "8b5204b7"
      },
      "source": [
        "#### 15.\tWhat is expand_as? Show an example of how it can be used to match the results of broadcasting."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6d54f2c",
      "metadata": {
        "id": "d6d54f2c"
      },
      "source": [
        "The expand_as method in PyTorch is a tensor method that facilitates expanding a tensor to match the size of another tensor. This functionality proves useful when you need to perform operations between tensors of different sizes that cannot be broadcasted.\n",
        "\n",
        "Here's an example demonstrating the usage of expand_as to align the results of broadcasting:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2755883",
      "metadata": {
        "id": "d2755883"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}