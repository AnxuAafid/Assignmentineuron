{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a248256",
   "metadata": {},
   "source": [
    "#### 1.\tIs it okay to initialize all the weights to the same value as long as that value is selected randomly using He initialization?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236101c0",
   "metadata": {},
   "source": [
    "It is acceptable to initialize all the weights to the same value, provided that the value is randomly selected using He initialization. He initialization is a particular form of random initialization method that guarantees a suitable mix of small and large values in the weights' initialization. This balanced initialization facilitates the optimization of the learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c5f184",
   "metadata": {},
   "source": [
    "#### 2.\tIs it okay to initialize the bias terms to 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a1f243",
   "metadata": {},
   "source": [
    "surely it is okay to initialize the bias terms to zero. Initializing the bias terms to zero is a popular approach as it allows the model to learn the bias more quickly during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf7aaa4",
   "metadata": {},
   "source": [
    "#### 3.\tName three advantages of the ELU activation function over ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249acbd8",
   "metadata": {},
   "source": [
    "a) The ELU activation function has a non-zero mean, which helps to reduce the vanishing gradients problem.\n",
    "b)The ELU activation function has a smoother transition from negative to positive values, which can help to improve the training process.\n",
    "c)The ELU activation function has a higher maximum output than ReLU, which can help to improve the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746cb7cc",
   "metadata": {},
   "source": [
    "#### 4.\tIn which cases would you want to use each of the following activation functions: ELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6d0ef2",
   "metadata": {},
   "source": [
    "ELU: ELU is best used in deep learning networks where the vanishing gradient problem is a concern.\n",
    "\n",
    "Leaky ReLU (and its variants): Leaky ReLU and its variants are best used in networks where the vanishing gradient problem is a concern, but where the non-zero mean of ELU is not desired.\n",
    "\n",
    "ReLU: ReLU is best used in networks where the vanishing gradient problem is not a concern and where a non-zero mean is not desired.\n",
    "\n",
    "Tanh: Tanh is best used in networks where a symmetric activation is desired and where the vanishing gradient problem is not a concern.\n",
    "\n",
    "Logistic: Logistic is best used in networks where a sigmoid activation is desired and where the vanishing gradient problem is not a concern.\n",
    "\n",
    "Softmax: Softmax is best used in networks where a probability distribution across multiple classes is desired."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332cbd41",
   "metadata": {},
   "source": [
    "#### 5.\tWhat may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using a MomentumOptimizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48ba38e",
   "metadata": {},
   "source": [
    "If the momentum hyperparameter is set too close to 1 (e.g., 0.99999), the model may not be able to make any updates to the weights during the training process as the gradients will be too small. This can lead to poor model performance and longer training times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e479268",
   "metadata": {},
   "source": [
    "#### 6.\tName three ways you can produce a sparse model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cb5729",
   "metadata": {},
   "source": [
    "1) Use a weight regularization technique such as L1 regularization, which encourages the model to set the weights for less important features to zero.\n",
    "\n",
    "2) Use a pruning technique such as magnitude pruning, which removes weights with small magnitudes from the model.\n",
    "\n",
    "3) Use a model architecture that produces sparse models such as a convolutional neural network, which has fewer parameters due to the shared weights across different convolutional layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71bd58a",
   "metadata": {},
   "source": [
    "#### 7.\tDoes dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49cd0ca",
   "metadata": {},
   "source": [
    "Brief answer:\n",
    "\n",
    "Dropout can potentially slow down the training process because it introduces randomness and requires more iterations for the network to converge.\n",
    "However, dropout does not slow down inference or the process of making predictions on new instances, as it is typically disabled during inference to allow for efficient and quick predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96e10b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
