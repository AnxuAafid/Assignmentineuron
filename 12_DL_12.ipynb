{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4a248256",
      "metadata": {
        "id": "4a248256"
      },
      "source": [
        "#### 1.\tHow does unsqueeze help us to solve certain broadcasting problems?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b20d2793",
      "metadata": {
        "id": "b20d2793"
      },
      "source": [
        "Unsqueeze is an operation commonly used in broadcasting to increase the dimensions of a tensor. Its purpose is to address situations where tensors of varying shapes need to be involved in element-wise or mathematical operations that require compatible tensor shapes.\n",
        "\n",
        "When we unsqueeze a tensor, we introduce additional dimensions to it, typically with a size of 1. This adjustment enhances alignment and compatibility with other tensors. The expanded tensor becomes capable of participating in element-wise operations or broadcasting operations alongside tensors of different shapes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5c5f184",
      "metadata": {
        "id": "f5c5f184"
      },
      "source": [
        "#### 2.\tHow can we use indexing to do the same operation as unsqueeze?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bcca88ff",
      "metadata": {
        "id": "bcca88ff"
      },
      "source": [
        "Indexing can be used to achieve a similar effect as unsqueeze by manipulating the shape and dimensions of a tensor. In particular, we can use indexing to insert new axes with a size of 1 into the tensor, achieving the expansion of dimensions. Here's how you can use indexing to accomplish this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0dc544f",
      "metadata": {
        "id": "d0dc544f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "A = np.array([1, 2, 3])  # Shape: (3,)\n",
        "\n",
        "# Insert a new axis at position 1\n",
        "A_expanded = A[:, np.newaxis]  # Shape: (3, 1)\n",
        "\n",
        "# Alternatively, use None to insert a new axis\n",
        "A_expanded = A[None, :]  # Shape: (1, 3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edf7aaa4",
      "metadata": {
        "id": "edf7aaa4"
      },
      "source": [
        "#### 3.\tHow do we show the actual contents of the memory used for a tensor?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eed724b9",
      "metadata": {
        "id": "eed724b9"
      },
      "source": [
        "In PyTorch, we can use the storage and tolist methods to show the actual contents of the memory used for a tensor.\n",
        "\n",
        "The storage method returns a one-dimensional tensor that contains the underlying storage of the tensor. This means that it returns a flattened version of the tensor's data, regardless of its shape.\n",
        "In general, showing the actual contents of the memory used for a tensor can be useful for debugging and understanding how the data is stored and manipulated in memory. However, it is important to note that the flattened storage may not always correspond to the tensor's shape and layout, especially for tensors with complex strides or non-contiguous memory."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "746cb7cc",
      "metadata": {
        "id": "746cb7cc"
      },
      "source": [
        "#### 4.\tWhen adding a vector of size 3 to a matrix of size 3Ã—3, are the elements of the vector added to each row or each column of the matrix? (Be sure to check your answer by running this code in a notebook.)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a0ddba8",
      "metadata": {
        "id": "1a0ddba8"
      },
      "source": [
        "When adding a vector of size 3 to a matrix of size 3x3, the elements of the vector are added to each row of the matrix. This is because broadcasting in PyTorch operates along the last dimensions of the tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "6ca7d5e0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ca7d5e0",
        "outputId": "1172bcef-b3bb-4b16-8aab-2b26c057ca5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2., 3., 4.],\n",
            "        [2., 3., 4.],\n",
            "        [2., 3., 4.]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "A = torch.ones((3, 3))\n",
        "\n",
        "B = torch.tensor([1, 2, 3])\n",
        "\n",
        "C = A + B\n",
        "\n",
        "print(C)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1a7e47b",
      "metadata": {
        "id": "c1a7e47b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "332cbd41",
      "metadata": {
        "id": "332cbd41"
      },
      "source": [
        "#### 5.\tDo broadcasting and expand_as result in increased memory use? Why or why not?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d0ad1a2",
      "metadata": {
        "id": "2d0ad1a2"
      },
      "source": [
        "Broadcasting and expand_as operations in tensor computations are memory-efficient because they do not create new copies of the data. Instead, they utilize the existing storage of the tensors and employ different views or shapes to perform the desired operations.\n",
        "\n",
        "The process of broadcasting involves implicitly expanding the tensor shapes to match each other along their non-singleton dimensions. This expansion does not involve duplicating the data; instead, it provides a different view of the same data with a modified shape. Similarly, expand_as creates a new tensor that shares the underlying storage of the input tensor but introduces additional dimensions of size 1, which are expanded to match the desired shape. Again, this results in a different view or shape of the same data, without duplicating it.\n",
        "\n",
        "As a result, broadcasting and expand_as operations are memory-efficient and save memory compared to explicitly copying the data. However, they may require additional memory for metadata and bookkeeping to manage the different views and shapes of the tensors.\n",
        "\n",
        "It's important to consider that while broadcasting and expand_as can be memory-efficient, they may impact performance, especially with large tensors and complex operations. In some cases, it might be more efficient to use explicit copying or manually reshape the tensors to avoid the overhead associated with broadcasting or expanding. Additionally, broadcasting and expand_as can lead to non-contiguous memory layouts, which can affect performance and memory access patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2603f558",
      "metadata": {
        "id": "2603f558"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "4e479268",
      "metadata": {
        "id": "4e479268"
      },
      "source": [
        "#### 6.\tImplement matmul using Einstein summation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71eb63b4",
      "metadata": {
        "id": "71eb63b4"
      },
      "outputs": [],
      "source": [
        "# Here is the code for same\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def matmul(A, B):\n",
        "\n",
        "    return (np.einsum('ij,jk->ik', A, B))\n",
        "\n",
        "A = np.array([[1, 2], [3, 4]])\n",
        "\n",
        "B = np.array([[5, 6], [7, 8]])\n",
        "\n",
        "C = matmul(A, B)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "539ddfc5",
      "metadata": {
        "id": "539ddfc5",
        "outputId": "811f7605-895b-4347-b4be-923d7ad6c3b4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[19, 22],\n",
              "       [43, 50]])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "C"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a08d2239",
      "metadata": {
        "id": "a08d2239"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "f71bd58a",
      "metadata": {
        "id": "f71bd58a"
      },
      "source": [
        "#### 7.\tWhat does a repeated index letter represent on the lefthand side of einsum?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca741b84",
      "metadata": {
        "id": "ca741b84"
      },
      "source": [
        "In Einstein summation notation, a repeated index letter on the lefthand side of the einsum function indicates summation over that index."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d16db75",
      "metadata": {
        "id": "0d16db75"
      },
      "source": [
        "#### 8.\tWhat are the three rules of Einstein summation notation? Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4a4ab27",
      "metadata": {
        "id": "b4a4ab27"
      },
      "source": [
        "The three rules of Einstein summation notation are as follows:\n",
        "\n",
        "Repeated indices are summed over: Whenever an index appears twice in a single term, it is assumed to be summed over. For example, in the expression A_ij B_jk, the index j is repeated and therefore summed over.\n",
        "\n",
        "Free indices are not summed over: An index that appears only once in a term is called a free index, and is not summed over. For example, in the expression A_ij x_j, the index i is a free index and is not summed over.\n",
        "\n",
        "The indices on the left-hand side of the einsum notation determine the indices in the output: The indices that appear in the left-hand side of the einsum notation represent the indices that will appear in the output. For example, in the expression np.einsum('ij,jk->ik', A, B), the resulting output will have indices i and k, since those are the indices that appear on the left-hand side of the einsum notation.\n",
        "\n",
        "These rules are necessary because Einstein summation notation allows for compact representation of matrix and tensor operations. The notation allows us to express complex operations in a concise way, without having to write out every single term explicitly. The rules ensure that the notation is unambiguous and consistent, and that the resulting output has the expected dimensions and indices."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "979e3af2",
      "metadata": {
        "id": "979e3af2"
      },
      "source": [
        "#### 9.\tWhat are the forward pass and backward pass of a neural network?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e4ec760",
      "metadata": {
        "id": "8e4ec760"
      },
      "source": [
        "Forward pass: In the forward pass, the input data is passed through the neural network to produce an output. Each neuron in the network receives input from the previous layer, applies an activation function to it, and outputs the result to the next layer. This process is repeated for each layer until the output layer produces the final result. During the forward pass, the weights and biases of the network are fixed and not updated.\n",
        "\n",
        "Backward pass: In the backward pass, the error in the output is calculated and propagated back through the network to adjust the weights and biases. This process is called backpropagation. The goal of backpropagation is to update the weights and biases in a way that reduces the error in the output. This is done by computing the gradient of the error with respect to each weight and bias in the network. The gradient is then used to update the weights and biases using an optimization algorithm such as gradient descent.\n",
        "\n",
        "The forward pass and backward pass are repeated multiple times during the training process, with each iteration updating the weights and biases of the network to improve its performance. The goal is to minimize the error between the network's predicted output and the true output."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd77bd31",
      "metadata": {
        "id": "bd77bd31"
      },
      "source": [
        "#### 10.\tWhy do we need to store some of the activations calculated for intermediate layers in the forward pass?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94cc07dd",
      "metadata": {
        "id": "94cc07dd"
      },
      "source": [
        "Storing the activations calculated for intermediate layers during the forward pass is essential for several reasons:\n",
        "\n",
        "Backpropagation: During the backward pass, the gradients of the loss with respect to the weights in each layer are computed by propagating the error backwards through the network. The gradients are computed using the activations and the derivatives of the activation functions in each layer. Therefore, the activations calculated during the forward pass are needed to perform the backward pass and update the weights of the network.\n",
        "\n",
        "Reuse of activations: In some cases, the activations calculated during the forward pass can be reused for multiple purposes. For example, in certain architectures such as residual networks, the activations from earlier layers are added to the activations from later layers. Storing the activations from earlier layers during the forward pass allows them to be reused later in the network.\n",
        "\n",
        "Debugging: Storing the activations from intermediate layers during the forward pass can be useful for debugging the network. By examining the values of the activations, we can gain insights into how the network is processing the input and identify potential problems.\n",
        "\n",
        "Visualization: Storing the activations from intermediate layers during the forward pass can also be useful for visualizing the features learned by the network. By examining the activations from different layers, we can gain insights into the types of patterns and features that the network is learning."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e860f8b5",
      "metadata": {
        "id": "e860f8b5"
      },
      "source": [
        "#### 11.\tWhat is the downside of having activations with a standard deviation too far away from 1?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f293b1e9",
      "metadata": {
        "id": "f293b1e9"
      },
      "source": [
        "When activations in a neural network have a standard deviation that is too far from 1, it can lead to issues such as vanishing or exploding gradients, slow convergence, limited discriminative power, and unstable training dynamics. Maintaining a reasonable standard deviation is important for stable and efficient training of the network."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c713869",
      "metadata": {
        "id": "8c713869"
      },
      "source": [
        "#### 12.\tHow can weight initialization help avoid this problem?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weight initialization helps avoid the problem of activations with a standard deviation too far from 1 by setting appropriate initial values for the weights in a neural network. It aims to balance the range of activations, break symmetry between neurons, prevent saturation, and normalize the scale of activations and gradients. Proper weight initialization promotes stable training, prevents vanishing or exploding gradients, and enables more efficient learning."
      ],
      "metadata": {
        "id": "DgY209516Yvu"
      },
      "id": "DgY209516Yvu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6268f8cd",
      "metadata": {
        "id": "6268f8cd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}